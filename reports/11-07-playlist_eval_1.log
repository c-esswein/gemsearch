nohup: ignoring input
2017-11-07 09:03:16,474 - summa.preprocessing.cleaner - INFO - 'pattern' package not found; tag filters are not available for English
2017-11-07 09:03:16,504 - __main__ - INFO - started playlist eval with config: {'dataDir': 'data/full_model/', 'outDir': 'data/rec/', 'SHOULD_GENERATE_GRAPH': False, 'SHOULD_INDEX_ES': False, 'TEST_PLAYLIST_SPLIT': 0.2, 'PRECISION_AT': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15], 'USE_USER_IN_QUERY': True, 'SHOULD_EMBED': False, 'ARGS': Namespace(album=True, genre=True, tags=True)}
2017-11-07 09:03:16,505 - __main__ - INFO - %%% playlist_eval runner... 
2017-11-07 09:03:16,715 - __main__ - INFO - %%% ge calc initializing... 
2017-11-07 09:03:47,326 - __main__ - INFO - %%% ge calc initializing done in 30.61s


2017-11-07 09:03:47,326 - __main__ - INFO - ------------- evaluation -------------
2017-11-07 09:03:47,327 - __main__ - INFO - %%% evaluation... 
2017-11-07 09:03:47,327 - gemsearch.evaluation.playlist_query_evaluator - INFO - Extract queries from playlist names (result is cached)
2017-11-07 09:10:47,739 - gemsearch.evaluation.playlist_query_evaluator - INFO - For evaluation 4139 of 4234 playlists are left
2017-11-07 09:10:47,743 - gemsearch.evaluation.playlist_query_evaluator - INFO - Started playlist evaluation with 4139 playlists
2017-11-07 12:27:43,861 - gemsearch.evaluation.playlist_query_evaluator - INFO - Playlist evaluation finished: total 4139 playlists (testsplit=0.2)
[{'key': 'rec_random_tracks@1',
  'values': {'avg_hits': 0.0,
             'avg_hits_on_has_hits': 0,
             'has_hits': 0,
             'precision': 0.0,
             'precision_on_has_hits': 0,
             'recall': 0.0}},
 {'key': 'rec_random_tracks@15',
  'values': {'avg_hits': 0.0016912297656438754,
             'avg_hits_on_has_hits': 1.0,
             'has_hits': 7,
             'precision': 0.00011274865104292501,
             'precision_on_has_hits': 0.06666666666666667,
             'recall': 2.2220847383980185e-05}},
 {'key': 'rec_random_tracks@10',
  'values': {'avg_hits': 0.001449625513409036,
             'avg_hits_on_has_hits': 1.0,
             'has_hits': 6,
             'precision': 0.0001449625513409036,
             'precision_on_has_hits': 0.09999999999999999,
             'recall': 2.0376540115011946e-05}},
 {'key': 'rec_random_tracks@5',
  'values': {'avg_hits': 0.000724812756704518,
             'avg_hits_on_has_hits': 1.0,
             'has_hits': 3,
             'precision': 0.00014496255134090362,
             'precision_on_has_hits': 0.20000000000000004,
             'recall': 2.909829329670871e-06}},
 {'key': 'rec_random_tracks@3',
  'values': {'avg_hits': 0.00048320850446967865,
             'avg_hits_on_has_hits': 1.0,
             'has_hits': 2,
             'precision': 0.0001610695014898929,
             'precision_on_has_hits': 0.3333333333333333,
             'recall': 4.693823373997662e-07}},
 {'key': 'rec_random_tracks@9',
  'values': {'avg_hits': 0.001449625513409036,
             'avg_hits_on_has_hits': 1.0,
             'has_hits': 6,
             'precision': 0.00016106950148989292,
             'precision_on_has_hits': 0.11111111111111112,
             'recall': 2.0376540115011946e-05}},
 {'key': 'rec_random_tracks@7',
  'values': {'avg_hits': 0.0012080212611741967,
             'avg_hits_on_has_hits': 1.0,
             'has_hits': 5,
             'precision': 0.00017257446588202807,
             'precision_on_has_hits': 0.14285714285714285,
             'recall': 3.119093526809139e-06}},
 {'key': 'rec_random_tracks@4',
  'values': {'avg_hits': 0.000724812756704518,
             'avg_hits_on_has_hits': 1.0,
             'has_hits': 3,
             'precision': 0.0001812031891761295,
             'precision_on_has_hits': 0.25,
             'recall': 2.909829329670871e-06}},
 {'key': 'rec_random_tracks@8',
  'values': {'avg_hits': 0.001449625513409036,
             'avg_hits_on_has_hits': 1.0,
             'has_hits': 6,
             'precision': 0.0001812031891761295,
             'precision_on_has_hits': 0.125,
             'recall': 2.0376540115011946e-05}},
 {'key': 'rec_random_tracks@6',
  'values': {'avg_hits': 0.0012080212611741967,
             'avg_hits_on_has_hits': 1.0,
             'has_hits': 5,
             'precision': 0.0002013368768623661,
             'precision_on_has_hits': 0.16666666666666666,
             'recall': 3.119093526809139e-06}},
 {'key': 'rec_random_tracks@2',
  'values': {'avg_hits': 0.00048320850446967865,
             'avg_hits_on_has_hits': 1.0,
             'has_hits': 2,
             'precision': 0.00024160425223483932,
             'precision_on_has_hits': 0.5,
             'recall': 4.693823373997662e-07}},
 {'key': 'rec_tracks_with_user@8',
  'values': {'avg_hits': 0.07465571394056536,
             'avg_hits_on_has_hits': 1.3981900452488687,
             'has_hits': 221,
             'precision': 0.00933196424257067,
             'precision_on_has_hits': 0.1747737556561086,
             'recall': 0.0016927213198340977}},
 {'key': 'rec_tracks_with_user@6',
  'values': {'avg_hits': 0.056052186518482726,
             'avg_hits_on_has_hits': 1.281767955801105,
             'has_hits': 181,
             'precision': 0.009342031086413785,
             'precision_on_has_hits': 0.21362799263351745,
             'recall': 0.001197948005740323}},
 {'key': 'rec_tracks_with_user@10',
  'values': {'avg_hits': 0.09374244986711766,
             'avg_hits_on_has_hits': 1.515625,
             'has_hits': 256,
             'precision': 0.009374244986711796,
             'precision_on_has_hits': 0.1515625000000005,
             'recall': 0.002082253341157811}},
 {'key': 'rec_tracks_with_user@9',
  'values': {'avg_hits': 0.08456148828219377,
             'avg_hits_on_has_hits': 1.4522821576763485,
             'has_hits': 241,
             'precision': 0.009395720920243756,
             'precision_on_has_hits': 0.16136468418626101,
             'recall': 0.001888653761042917}},
 {'key': 'rec_tracks_with_user@15',
  'values': {'avg_hits': 0.14230490456632036,
             'avg_hits_on_has_hits': 1.7794561933534743,
             'has_hits': 331,
             'precision': 0.009486993637754713,
             'precision_on_has_hits': 0.11863041289023189,
             'recall': 0.0030498504980729954}},
 {'key': 'rec_tracks_with_user@5',
  'values': {'avg_hits': 0.04783764194249819,
             'avg_hits_on_has_hits': 1.2452830188679245,
             'has_hits': 159,
             'precision': 0.009567528388499641,
             'precision_on_has_hits': 0.24905660377358502,
             'recall': 0.0010260172017857927}},
 {'key': 'rec_tracks_with_user@7',
  'values': {'avg_hits': 0.06716598212128533,
             'avg_hits_on_has_hits': 1.3560975609756099,
             'has_hits': 205,
             'precision': 0.00959514030304077,
             'precision_on_has_hits': 0.19372822299651582,
             'recall': 0.0014928210730050155}},
 {'key': 'rec_tracks_with_user@4',
  'values': {'avg_hits': 0.03841507610533945,
             'avg_hits_on_has_hits': 1.1865671641791045,
             'has_hits': 134,
             'precision': 0.009603769026334863,
             'precision_on_has_hits': 0.2966417910447761,
             'recall': 0.0008724562673805863}},
 {'key': 'rec_tracks_with_user@3',
  'values': {'avg_hits': 0.030200531529354917,
             'avg_hits_on_has_hits': 1.1363636363636365,
             'has_hits': 110,
             'precision': 0.010066843843118307,
             'precision_on_has_hits': 0.37878787878787884,
             'recall': 0.000722766796127925}},
 {'key': 'rec_tracks_with_user@2',
  'values': {'avg_hits': 0.02101956994443102,
             'avg_hits_on_has_hits': 1.1012658227848102,
             'has_hits': 79,
             'precision': 0.01050978497221551,
             'precision_on_has_hits': 0.5506329113924051,
             'recall': 0.0005308984998784413}},
 {'key': 'rec_tracks_with_user@1',
  'values': {'avg_hits': 0.01087219135056777,
             'avg_hits_on_has_hits': 1.0,
             'has_hits': 45,
             'precision': 0.01087219135056777,
             'precision_on_has_hits': 1.0,
             'recall': 0.00024326743216137255}},
 {'key': 'rec_query_tracks_with_user_mean@15',
  'values': {'avg_hits': 0.4373036965450592,
             'avg_hits_on_has_hits': 2.4896836313617605,
             'has_hits': 727,
             'precision': 0.029153579769670663,
             'precision_on_has_hits': 0.165978908757451,
             'recall': 0.01917844099324158}},
 {'key': 'rec_query_tracks_with_user@15',
  'values': {'avg_hits': 0.4373036965450592,
             'avg_hits_on_has_hits': 2.4896836313617605,
             'has_hits': 727,
             'precision': 0.029153579769670663,
             'precision_on_has_hits': 0.165978908757451,
             'recall': 0.01917844099324158}},
 {'key': 'rec_query_tracks_with_user_mean@10',
  'values': {'avg_hits': 0.3186760086977531,
             'avg_hits_on_has_hits': 2.1622950819672133,
             'has_hits': 610,
             'precision': 0.03186760086977515,
             'precision_on_has_hits': 0.21622950819672024,
             'recall': 0.014331652728788113}},
 {'key': 'rec_query_tracks_with_user@10',
  'values': {'avg_hits': 0.3186760086977531,
             'avg_hits_on_has_hits': 2.1622950819672133,
             'has_hits': 610,
             'precision': 0.03186760086977515,
             'precision_on_has_hits': 0.21622950819672024,
             'recall': 0.014331652728788113}},
 {'key': 'rec_query_tracks_with_user_mean@9',
  'values': {'avg_hits': 0.2870258516549891,
             'avg_hits_on_has_hits': 2.055363321799308,
             'has_hits': 578,
             'precision': 0.03189176129499904,
             'precision_on_has_hits': 0.2283737024221471,
             'recall': 0.012959041347564709}},
 {'key': 'rec_query_tracks_with_user@9',
  'values': {'avg_hits': 0.2870258516549891,
             'avg_hits_on_has_hits': 2.055363321799308,
             'has_hits': 578,
             'precision': 0.03189176129499904,
             'precision_on_has_hits': 0.2283737024221471,
             'recall': 0.012959041347564709}},
 {'key': 'rec_query_tracks_with_user_mean@8',
  'values': {'avg_hits': 0.26310703068374003,
             'avg_hits_on_has_hits': 1.9764065335753176,
             'has_hits': 551,
             'precision': 0.032888378835467504,
             'precision_on_has_hits': 0.2470508166969147,
             'recall': 0.012016021114969317}},
 {'key': 'rec_query_tracks_with_user@8',
  'values': {'avg_hits': 0.26310703068374003,
             'avg_hits_on_has_hits': 1.9764065335753176,
             'has_hits': 551,
             'precision': 0.032888378835467504,
             'precision_on_has_hits': 0.2470508166969147,
             'recall': 0.012016021114969317}},
 {'key': 'rec_query_tracks_with_user_mean@7',
  'values': {'avg_hits': 0.23363131191108963,
             'avg_hits_on_has_hits': 1.8667953667953667,
             'has_hits': 518,
             'precision': 0.033375901701584124,
             'precision_on_has_hits': 0.2666850523993372,
             'recall': 0.010636423756534966}},
 {'key': 'rec_query_tracks_with_user@7',
  'values': {'avg_hits': 0.23363131191108963,
             'avg_hits_on_has_hits': 1.8667953667953667,
             'has_hits': 518,
             'precision': 0.033375901701584124,
             'precision_on_has_hits': 0.2666850523993372,
             'recall': 0.010636423756534966}},
 {'key': 'rec_query_tracks_with_user_mean@6',
  'values': {'avg_hits': 0.20802126117419667,
             'avg_hits_on_has_hits': 1.75,
             'has_hits': 492,
             'precision': 0.03467021019569948,
             'precision_on_has_hits': 0.29166666666666696,
             'recall': 0.009544133073515992}},
 {'key': 'rec_query_tracks_with_user@6',
  'values': {'avg_hits': 0.20802126117419667,
             'avg_hits_on_has_hits': 1.75,
             'has_hits': 492,
             'precision': 0.03467021019569948,
             'precision_on_has_hits': 0.29166666666666696,
             'recall': 0.009544133073515992}},
 {'key': 'rec_query_tracks_with_user_mean@5',
  'values': {'avg_hits': 0.1807199806716598,
             'avg_hits_on_has_hits': 1.636761487964989,
             'has_hits': 457,
             'precision': 0.03614399613433199,
             'precision_on_has_hits': 0.32735229759299805,
             'recall': 0.008513234851020077}},
 {'key': 'rec_query_tracks_with_user@5',
  'values': {'avg_hits': 0.1807199806716598,
             'avg_hits_on_has_hits': 1.636761487964989,
             'has_hits': 457,
             'precision': 0.03614399613433199,
             'precision_on_has_hits': 0.32735229759299805,
             'recall': 0.008513234851020077}},
 {'key': 'rec_first_two_query_tracks_with_user@15',
  'values': {'avg_hits': 0.5571394056535395,
             'avg_hits_on_has_hits': 2.6845168800931316,
             'has_hits': 859,
             'precision': 0.03714262704356921,
             'precision_on_has_hits': 0.17896779200620833,
             'recall': 0.02689489442803548}},
 {'key': 'rec_query_tracks_with_user_mean@4',
  'values': {'avg_hits': 0.1529354916646533,
             'avg_hits_on_has_hits': 1.5253012048192771,
             'has_hits': 415,
             'precision': 0.03823387291616333,
             'precision_on_has_hits': 0.3813253012048193,
             'recall': 0.007296107726513893}},
 {'key': 'rec_query_tracks_with_user@4',
  'values': {'avg_hits': 0.1529354916646533,
             'avg_hits_on_has_hits': 1.5253012048192771,
             'has_hits': 415,
             'precision': 0.03823387291616333,
             'precision_on_has_hits': 0.3813253012048193,
             'recall': 0.007296107726513893}},
 {'key': 'rec_query_tracks_with_user_mean@3',
  'values': {'avg_hits': 0.12176854312635903,
             'avg_hits_on_has_hits': 1.3808219178082193,
             'has_hits': 365,
             'precision': 0.04058951437545304,
             'precision_on_has_hits': 0.46027397260274006,
             'recall': 0.005939040583051307}},
 {'key': 'rec_query_tracks_with_user@3',
  'values': {'avg_hits': 0.12176854312635903,
             'avg_hits_on_has_hits': 1.3808219178082193,
             'has_hits': 365,
             'precision': 0.04058951437545304,
             'precision_on_has_hits': 0.46027397260274006,
             'recall': 0.005939040583051307}},
 {'key': 'rec_first_two_query_tracks_with_user@10',
  'values': {'avg_hits': 0.4097608117902875,
             'avg_hits_on_has_hits': 2.320109439124487,
             'has_hits': 731,
             'precision': 0.04097608117902847,
             'precision_on_has_hits': 0.2320109439124471,
             'recall': 0.02022177104250338}},
 {'key': 'rec_first_two_query_tracks_with_user@9',
  'values': {'avg_hits': 0.37593621647741,
             'avg_hits_on_has_hits': 2.251808972503618,
             'has_hits': 691,
             'precision': 0.041770690719712567,
             'precision_on_has_hits': 0.2502009969448485,
             'recall': 0.018572216646743078}},
 {'key': 'rec_first_two_query_tracks_with_user@8',
  'values': {'avg_hits': 0.342836433921237,
             'avg_hits_on_has_hits': 2.1370481927710845,
             'has_hits': 664,
             'precision': 0.04285455424015463,
             'precision_on_has_hits': 0.26713102409638556,
             'recall': 0.016952255577367112}},
 {'key': 'rec_first_two_query_tracks_with_user@7',
  'values': {'avg_hits': 0.3049045663203672,
             'avg_hits_on_has_hits': 2.0192,
             'has_hits': 625,
             'precision': 0.04355779518862365,
             'precision_on_has_hits': 0.28845714285714125,
             'recall': 0.01519414654640105}},
 {'key': 'rec_first_two_query_tracks_with_user@6',
  'values': {'avg_hits': 0.27011355399855036,
             'avg_hits_on_has_hits': 1.898132427843803,
             'has_hits': 589,
             'precision': 0.04501892566642499,
             'precision_on_has_hits': 0.3163554046406334,
             'recall': 0.01382721033865584}},
 {'key': 'rec_query_tracks_with_user_mean@2',
  'values': {'avg_hits': 0.09108480309253443,
             'avg_hits_on_has_hits': 1.2401315789473684,
             'has_hits': 304,
             'precision': 0.045542401546267215,
             'precision_on_has_hits': 0.6200657894736842,
             'recall': 0.004579471207063515}},
 {'key': 'rec_query_tracks_with_user@2',
  'values': {'avg_hits': 0.09108480309253443,
             'avg_hits_on_has_hits': 1.2401315789473684,
             'has_hits': 304,
             'precision': 0.045542401546267215,
             'precision_on_has_hits': 0.6200657894736842,
             'recall': 0.004579471207063515}},
 {'key': 'rec_first_two_query_tracks_with_user@5',
  'values': {'avg_hits': 0.23435612466779415,
             'avg_hits_on_has_hits': 1.7604355716878404,
             'has_hits': 551,
             'precision': 0.04687122493355869,
             'precision_on_has_hits': 0.352087114337567,
             'recall': 0.01215919035557731}},
 {'key': 'rec_first_two_query_tracks_with_user@4',
  'values': {'avg_hits': 0.1983570910848031,
             'avg_hits_on_has_hits': 1.6257425742574256,
             'has_hits': 505,
             'precision': 0.049589272771200774,
             'precision_on_has_hits': 0.4064356435643564,
             'recall': 0.010401428174953622}},
 {'key': 'rec_query_tracks_with_user_mean@1',
  'values': {'avg_hits': 0.05073689296931626,
             'avg_hits_on_has_hits': 1.0,
             'has_hits': 210,
             'precision': 0.05073689296931626,
             'precision_on_has_hits': 1.0,
             'recall': 0.0026904415224316795}},
 {'key': 'rec_query_tracks_with_user@1',
  'values': {'avg_hits': 0.05073689296931626,
             'avg_hits_on_has_hits': 1.0,
             'has_hits': 210,
             'precision': 0.05073689296931626,
             'precision_on_has_hits': 1.0,
             'recall': 0.0026904415224316795}},
 {'key': 'rec_first_two_query_tracks_with_user@3',
  'values': {'avg_hits': 0.15800918096158492,
             'avg_hits_on_has_hits': 1.4533333333333334,
             'has_hits': 450,
             'precision': 0.05266972698719512,
             'precision_on_has_hits': 0.4844444444444458,
             'recall': 0.008322627904561456}},
 {'key': 'rec_first_two_query_tracks_with_user@2',
  'values': {'avg_hits': 0.11427881130707901,
             'avg_hits_on_has_hits': 1.2480211081794195,
             'has_hits': 379,
             'precision': 0.057139405653539505,
             'precision_on_has_hits': 0.6240105540897097,
             'recall': 0.006203868213165407}},
 {'key': 'rec_first_two_query_tracks_with_user@1',
  'values': {'avg_hits': 0.06281710558105823,
             'avg_hits_on_has_hits': 1.0,
             'has_hits': 260,
             'precision': 0.06281710558105823,
             'precision_on_has_hits': 1.0,
             'recall': 0.0035200505017292166}},
 {'key': 'rec_query_tracks_with_user_scaled@15',
  'values': {'avg_hits': 1.0744141096883306,
             'avg_hits_on_has_hits': 4.013537906137184,
             'has_hits': 1108,
             'precision': 0.07162760731255506,
             'precision_on_has_hits': 0.2675691937424778,
             'recall': 0.06114505881569411}},
 {'key': 'rec_query_tracks_with_user_scaled@10',
  'values': {'avg_hits': 0.7637110413143271,
             'avg_hits_on_has_hits': 3.2621259029927763,
             'has_hits': 969,
             'precision': 0.07637110413143255,
             'precision_on_has_hits': 0.3262125902992769,
             'recall': 0.04468403566014511}},
 {'key': 'rec_query_tracks_with_user_scaled@9',
  'values': {'avg_hits': 0.6924377869050495,
             'avg_hits_on_has_hits': 3.068522483940043,
             'has_hits': 934,
             'precision': 0.07693753187833877,
             'precision_on_has_hits': 0.34094694266000447,
             'recall': 0.04072060402200141}},
 {'key': 'rec_query_tracks_with_user_scaled@8',
  'values': {'avg_hits': 0.6209229282435371,
             'avg_hits_on_has_hits': 2.8876404494382024,
             'has_hits': 890,
             'precision': 0.07761536603044214,
             'precision_on_has_hits': 0.3609550561797753,
             'recall': 0.03634839207870328}},
 {'key': 'rec_query_tracks_with_user_scaled@7',
  'values': {'avg_hits': 0.5479584440686156,
             'avg_hits_on_has_hits': 2.646441073512252,
             'has_hits': 857,
             'precision': 0.07827977772408806,
             'precision_on_has_hits': 0.3780630105017509,
             'recall': 0.032018348415505195}},
 {'key': 'rec_query_tracks_with_user_scaled@6',
  'values': {'avg_hits': 0.47209470886687604,
             'avg_hits_on_has_hits': 2.415327564894932,
             'has_hits': 809,
             'precision': 0.07868245147781266,
             'precision_on_has_hits': 0.4025545941491553,
             'recall': 0.027711284222687427}},
 {'key': 'rec_query_tracks_with_user_scaled@5',
  'values': {'avg_hits': 0.4020294757187727,
             'avg_hits_on_has_hits': 2.1780104712041886,
             'has_hits': 764,
             'precision': 0.08040589514375401,
             'precision_on_has_hits': 0.4356020942408349,
             'recall': 0.023659510909228243}},
 {'key': 'rec_query_tracks_with_user_scaled@4',
  'values': {'avg_hits': 0.32785697028267696,
             'avg_hits_on_has_hits': 1.9413447782546496,
             'has_hits': 699,
             'precision': 0.08196424257066924,
             'precision_on_has_hits': 0.4853361945636624,
             'recall': 0.019310608891428174}},
 {'key': 'rec_query_tracks_with_user_scaled@3',
  'values': {'avg_hits': 0.2507852138197632,
             'avg_hits_on_has_hits': 1.6741935483870967,
             'has_hits': 620,
             'precision': 0.08359507127325434,
             'precision_on_has_hits': 0.5580645161290319,
             'recall': 0.014742152114174406}},
 {'key': 'rec_first_two_query_tracks_with_user_scaled@15',
  'values': {'avg_hits': 1.2701135539985504,
             'avg_hits_on_has_hits': 4.113458528951487,
             'has_hits': 1278,
             'precision': 0.08467423693323617,
             'precision_on_has_hits': 0.2742305685967641,
             'recall': 0.0753583800139145}},
 {'key': 'rec_query_tracks_with_user_scaled@2',
  'values': {'avg_hits': 0.17226383184344043,
             'avg_hits_on_has_hits': 1.3898635477582846,
             'has_hits': 513,
             'precision': 0.08613191592172022,
             'precision_on_has_hits': 0.6949317738791423,
             'recall': 0.010047759311823546}},
 {'key': 'rec_query_tracks_with_user_scaled@1',
  'values': {'avg_hits': 0.08987678183136023,
             'avg_hits_on_has_hits': 1.0,
             'has_hits': 372,
             'precision': 0.08987678183136023,
             'precision_on_has_hits': 1.0,
             'recall': 0.005404422038915004}},
 {'key': 'rec_first_two_query_tracks_with_user_scaled@10',
  'values': {'avg_hits': 0.9139888862043972,
             'avg_hits_on_has_hits': 3.3448275862068964,
             'has_hits': 1131,
             'precision': 0.09139888862043996,
             'precision_on_has_hits': 0.33448275862069055,
             'recall': 0.056692115757072684}},
 {'key': 'rec_query_tracks@15',
  'values': {'avg_hits': 1.3773858419908191,
             'avg_hits_on_has_hits': 5.7644084934277044,
             'has_hits': 989,
             'precision': 0.09182572279938767,
             'precision_on_has_hits': 0.38429389956184584,
             'recall': 0.0937047537979649}},
 {'key': 'rec_first_two_query_tracks_with_user_scaled@9',
  'values': {'avg_hits': 0.8303938149311428,
             'avg_hits_on_has_hits': 3.185356811862836,
             'has_hits': 1079,
             'precision': 0.09226597943679321,
             'precision_on_has_hits': 0.35392853465142454,
             'recall': 0.05169931282001401}},
 {'key': 'rec_first_two_query_tracks@15',
  'values': {'avg_hits': 1.396714182169606,
             'avg_hits_on_has_hits': 5.397759103641457,
             'has_hits': 1071,
             'precision': 0.09311427881130667,
             'precision_on_has_hits': 0.3598506069094289,
             'recall': 0.09484979923099253}},
 {'key': 'rec_first_two_query_tracks_mean@15',
  'values': {'avg_hits': 1.396714182169606,
             'avg_hits_on_has_hits': 5.397759103641457,
             'has_hits': 1071,
             'precision': 0.09311427881130667,
             'precision_on_has_hits': 0.3598506069094289,
             'recall': 0.09484979923099253}},
 {'key': 'rec_first_two_query_tracks_with_user_scaled@8',
  'values': {'avg_hits': 0.7494563904324716,
             'avg_hits_on_has_hits': 2.9798270893371757,
             'has_hits': 1041,
             'precision': 0.09368204880405895,
             'precision_on_has_hits': 0.37247838616714696,
             'recall': 0.04692011827624703}},
 {'key': 'rec_first_two_query_tracks_with_user_scaled@7',
  'values': {'avg_hits': 0.6627204638801643,
             'avg_hits_on_has_hits': 2.7320717131474104,
             'has_hits': 1004,
             'precision': 0.09467435198288111,
             'precision_on_has_hits': 0.3902959590210607,
             'recall': 0.041503366746869254}},
 {'key': 'rec_first_two_query_tracks_with_user_scaled@6',
  'values': {'avg_hits': 0.5738100990577434,
             'avg_hits_on_has_hits': 2.5,
             'has_hits': 950,
             'precision': 0.09563501650962412,
             'precision_on_has_hits': 0.41666666666666763,
             'recall': 0.035867856317165006}},
 {'key': 'rec_first_two_query_tracks_with_user_scaled@5',
  'values': {'avg_hits': 0.48610775549649676,
             'avg_hits_on_has_hits': 2.2581369248035914,
             'has_hits': 891,
             'precision': 0.09722155109929868,
             'precision_on_has_hits': 0.4516273849607152,
             'recall': 0.030236566516914382}},
 {'key': 'rec_first_two_query_tracks_with_user_scaled@4',
  'values': {'avg_hits': 0.3935733268905533,
             'avg_hits_on_has_hits': 1.9817518248175183,
             'has_hits': 822,
             'precision': 0.09839333172263832,
             'precision_on_has_hits': 0.4954379562043796,
             'recall': 0.024679106839509333}},
 {'key': 'rec_first_two_query_tracks_with_user_scaled@3',
  'values': {'avg_hits': 0.29886446001449624,
             'avg_hits_on_has_hits': 1.7156726768377253,
             'has_hits': 721,
             'precision': 0.09962148667149853,
             'precision_on_has_hits': 0.5718908922792405,
             'recall': 0.01879678211978579}},
 {'key': 'rec_query_tracks@10',
  'values': {'avg_hits': 1.001449625513409,
             'avg_hits_on_has_hits': 4.68361581920904,
             'has_hits': 885,
             'precision': 0.10014496255134095,
             'precision_on_has_hits': 0.46836158192090416,
             'recall': 0.07116789604551385}},
 {'key': 'rec_first_two_query_tracks@10',
  'values': {'avg_hits': 1.0101473785938633,
             'avg_hits_on_has_hits': 4.350676378772112,
             'has_hits': 961,
             'precision': 0.10101473785938646,
             'precision_on_has_hits': 0.4350676378772118,
             'recall': 0.07139650175791902}},
 {'key': 'rec_first_two_query_tracks_mean@10',
  'values': {'avg_hits': 1.0101473785938633,
             'avg_hits_on_has_hits': 4.350676378772112,
             'has_hits': 961,
             'precision': 0.10101473785938646,
             'precision_on_has_hits': 0.4350676378772118,
             'recall': 0.07139650175791902}},
 {'key': 'rec_query_tracks@9',
  'values': {'avg_hits': 0.9106064266731094,
             'avg_hits_on_has_hits': 4.408187134502924,
             'has_hits': 855,
             'precision': 0.10117849185256748,
             'precision_on_has_hits': 0.48979857050032377,
             'recall': 0.06520876606536874}},
 {'key': 'rec_first_two_query_tracks@9',
  'values': {'avg_hits': 0.9132640734476927,
             'avg_hits_on_has_hits': 4.038461538461538,
             'has_hits': 936,
             'precision': 0.10147378593863213,
             'precision_on_has_hits': 0.44871794871794696,
             'recall': 0.0651567346338595}},
 {'key': 'rec_first_two_query_tracks_mean@9',
  'values': {'avg_hits': 0.9132640734476927,
             'avg_hits_on_has_hits': 4.038461538461538,
             'has_hits': 936,
             'precision': 0.10147378593863213,
             'precision_on_has_hits': 0.44871794871794696,
             'recall': 0.0651567346338595}},
 {'key': 'rec_first_two_query_tracks_with_user_scaled@2',
  'values': {'avg_hits': 0.2034307803817347,
             'avg_hits_on_has_hits': 1.3940397350993377,
             'has_hits': 604,
             'precision': 0.10171539019086735,
             'precision_on_has_hits': 0.6970198675496688,
             'recall': 0.012652329258279544}},
 {'key': 'rec_query_tracks@8',
  'values': {'avg_hits': 0.8183136023194009,
             'avg_hits_on_has_hits': 4.0562874251497005,
             'has_hits': 835,
             'precision': 0.10228920028992511,
             'precision_on_has_hits': 0.5070359281437126,
             'recall': 0.058942984243114956}},
 {'key': 'rec_first_two_query_tracks@8',
  'values': {'avg_hits': 0.8219376661029234,
             'avg_hits_on_has_hits': 3.7302631578947367,
             'has_hits': 912,
             'precision': 0.10274220826286543,
             'precision_on_has_hits': 0.4662828947368421,
             'recall': 0.059083232874923425}},
 {'key': 'rec_first_two_query_tracks_mean@8',
  'values': {'avg_hits': 0.8219376661029234,
             'avg_hits_on_has_hits': 3.7302631578947367,
             'has_hits': 912,
             'precision': 0.10274220826286543,
             'precision_on_has_hits': 0.4662828947368421,
             'recall': 0.059083232874923425}},
 {'key': 'rec_query_tracks@7',
  'values': {'avg_hits': 0.7194974631553516,
             'avg_hits_on_has_hits': 3.7132169576059852,
             'has_hits': 802,
             'precision': 0.10278535187933613,
             'precision_on_has_hits': 0.5304595653722846,
             'recall': 0.052065170049716857}},
 {'key': 'rec_first_two_query_tracks@7',
  'values': {'avg_hits': 0.7236047354433438,
             'avg_hits_on_has_hits': 3.4385763490241104,
             'has_hits': 871,
             'precision': 0.1033721050633352,
             'precision_on_has_hits': 0.49122519271773185,
             'recall': 0.05234213354882041}},
 {'key': 'rec_first_two_query_tracks_mean@7',
  'values': {'avg_hits': 0.7236047354433438,
             'avg_hits_on_has_hits': 3.4385763490241104,
             'has_hits': 871,
             'precision': 0.1033721050633352,
             'precision_on_has_hits': 0.49122519271773185,
             'recall': 0.05234213354882041}},
 {'key': 'rec_query_tracks@6',
  'values': {'avg_hits': 0.621164532495772,
             'avg_hits_on_has_hits': 3.3346303501945527,
             'has_hits': 771,
             'precision': 0.10352742208262873,
             'precision_on_has_hits': 0.5557717250324259,
             'recall': 0.04519019687646811}},
 {'key': 'rec_query_tracks@5',
  'values': {'avg_hits': 0.5221067890794878,
             'avg_hits_on_has_hits': 2.9123989218328843,
             'has_hits': 742,
             'precision': 0.10442135781589719,
             'precision_on_has_hits': 0.5824797843665748,
             'recall': 0.03822476109872352}},
 {'key': 'rec_first_two_query_tracks@6',
  'values': {'avg_hits': 0.6279294515583475,
             'avg_hits_on_has_hits': 3.138888888888889,
             'has_hits': 828,
             'precision': 0.10465490859305812,
             'precision_on_has_hits': 0.5231481481481493,
             'recall': 0.04558593297552516}},
 {'key': 'rec_first_two_query_tracks_mean@6',
  'values': {'avg_hits': 0.6279294515583475,
             'avg_hits_on_has_hits': 3.138888888888889,
             'has_hits': 828,
             'precision': 0.10465490859305812,
             'precision_on_has_hits': 0.5231481481481493,
             'recall': 0.04558593297552516}},
 {'key': 'rec_first_two_query_tracks@5',
  'values': {'avg_hits': 0.5247644358540711,
             'avg_hits_on_has_hits': 2.7989690721649483,
             'has_hits': 776,
             'precision': 0.10495288717081379,
             'precision_on_has_hits': 0.5597938144329875,
             'recall': 0.03825432214627272}},
 {'key': 'rec_first_two_query_tracks_mean@5',
  'values': {'avg_hits': 0.5247644358540711,
             'avg_hits_on_has_hits': 2.7989690721649483,
             'has_hits': 776,
             'precision': 0.10495288717081379,
             'precision_on_has_hits': 0.5597938144329875,
             'recall': 0.03825432214627272}},
 {'key': 'rec_query_tracks@4',
  'values': {'avg_hits': 0.4220826286542643,
             'avg_hits_on_has_hits': 2.4957142857142856,
             'has_hits': 700,
             'precision': 0.10552065716356608,
             'precision_on_has_hits': 0.6239285714285714,
             'recall': 0.03091790174682408}},
 {'key': 'rec_first_two_query_tracks@4',
  'values': {'avg_hits': 0.4232906499154385,
             'avg_hits_on_has_hits': 2.3836734693877553,
             'has_hits': 735,
             'precision': 0.10582266247885963,
             'precision_on_has_hits': 0.5959183673469388,
             'recall': 0.030742893322210262}},
 {'key': 'rec_first_two_query_tracks_mean@4',
  'values': {'avg_hits': 0.4232906499154385,
             'avg_hits_on_has_hits': 2.3836734693877553,
             'has_hits': 735,
             'precision': 0.10582266247885963,
             'precision_on_has_hits': 0.5959183673469388,
             'recall': 0.030742893322210262}},
 {'key': 'rec_first_two_query_tracks_with_user_scaled@1',
  'values': {'avg_hits': 0.10606426673109447,
             'avg_hits_on_has_hits': 1.0,
             'has_hits': 439,
             'precision': 0.10606426673109447,
             'precision_on_has_hits': 1.0,
             'recall': 0.006609020276932279}},
 {'key': 'rec_first_two_query_tracks@3',
  'values': {'avg_hits': 0.32085044696786663,
             'avg_hits_on_has_hits': 1.9615952732644018,
             'has_hits': 677,
             'precision': 0.10695014898928858,
             'precision_on_has_hits': 0.6538650910881321,
             'recall': 0.023267981033275632}},
 {'key': 'rec_first_two_query_tracks_mean@3',
  'values': {'avg_hits': 0.32085044696786663,
             'avg_hits_on_has_hits': 1.9615952732644018,
             'has_hits': 677,
             'precision': 0.10695014898928858,
             'precision_on_has_hits': 0.6538650910881321,
             'recall': 0.023267981033275632}},
 {'key': 'rec_query_tracks@3',
  'values': {'avg_hits': 0.3215752597245711,
             'avg_hits_on_has_hits': 2.0476923076923077,
             'has_hits': 650,
             'precision': 0.10719175324152351,
             'precision_on_has_hits': 0.6825641025641013,
             'recall': 0.023550783559814364}},
 {'key': 'rec_query_tracks@2',
  'values': {'avg_hits': 0.21696061850688572,
             'avg_hits_on_has_hits': 1.5563258232235702,
             'has_hits': 577,
             'precision': 0.10848030925344286,
             'precision_on_has_hits': 0.7781629116117851,
             'recall': 0.01577725541408115}},
 {'key': 'rec_first_two_query_tracks@2',
  'values': {'avg_hits': 0.22034307803817346,
             'avg_hits_on_has_hits': 1.5276381909547738,
             'has_hits': 597,
             'precision': 0.11017153901908673,
             'precision_on_has_hits': 0.7638190954773869,
             'recall': 0.015934970015489665}},
 {'key': 'rec_first_two_query_tracks_mean@2',
  'values': {'avg_hits': 0.22034307803817346,
             'avg_hits_on_has_hits': 1.5276381909547738,
             'has_hits': 597,
             'precision': 0.11017153901908673,
             'precision_on_has_hits': 0.7638190954773869,
             'recall': 0.015934970015489665}},
 {'key': 'rec_first_two_query_tracks@1',
  'values': {'avg_hits': 0.11307079004590481,
             'avg_hits_on_has_hits': 1.0,
             'has_hits': 468,
             'precision': 0.11307079004590481,
             'precision_on_has_hits': 1.0,
             'recall': 0.008140748439739528}},
 {'key': 'rec_first_two_query_tracks_mean@1',
  'values': {'avg_hits': 0.11307079004590481,
             'avg_hits_on_has_hits': 1.0,
             'has_hits': 468,
             'precision': 0.11307079004590481,
             'precision_on_has_hits': 1.0,
             'recall': 0.008140748439739528}},
 {'key': 'rec_query_tracks@1',
  'values': {'avg_hits': 0.1166948538294274,
             'avg_hits_on_has_hits': 1.0,
             'has_hits': 483,
             'precision': 0.1166948538294274,
             'precision_on_has_hits': 1.0,
             'recall': 0.008542916821027993}}]
2017-11-07 12:27:43,881 - __main__ - INFO - %%% evaluation done in 12236.554s


------------- done -------------
2017-11-07 12:27:43,881 - __main__ - INFO - %%% playlist_eval runner done in 12267.376s


